{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Plant Images\n",
    "## Week 7 Assignment\n",
    "### Ravi Kasarla\n",
    "```\n",
    "Creation Date: 13-Oct-2019\n",
    "Last Update Date: 13-Oct-2019\n",
    "```\n",
    "#### Useful Links:\n",
    "1. [Classify Plant Images](https://www.kaggle.com/datduyn/2-layer-net-on-weeds-discriminant/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'rcParams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-78a2c8c1d6b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m from .transforms import (Bbox, IdentityTransform, Transform, TransformedBbox,\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'rcParams'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (19.0, 17.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'../dataset/'\n",
    "classes = ['broadleaf', 'grass', 'soil', 'soybean'] \n",
    "\n",
    "num_file = 1100 \n",
    "all_files = [] \n",
    "num_data =num_file*len(classes)\n",
    "Y = np.zeros(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fb54bfa4b273>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mall_files\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/*.tif'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_file\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_file\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;31m# label all classes with int [0.. len(classes)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "for i, cls in enumerate(classes):\n",
    "    all_files += [f for f in glob.glob(data_dir+cls+'/*.tif')][:num_file]\n",
    "    Y[i*num_file:(i+1)*num_file] = i # label all classes with int [0.. len(classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimension\n",
    "im_width = 200\n",
    "im_height = 200 \n",
    "im_channel = 3\n",
    "dim = im_width * im_height * im_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of six failed: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 384, in superreload\n",
      "    def update_generic(a, b):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 323, in update_generic\n",
      "    def update_class(old, new):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 275, in update_class\n",
      "    \"\"\"\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\six.py\", line 93, in __get__\n",
      "    setattr(obj, self.name, result)  # Invokes __set__.\n",
      "AttributeError: 'NoneType' object has no attribute 'cStringIO'\n",
      "]\n",
      "[autoreload of traitlets.config.application failed: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 368, in superreload\n",
      "    return isinstance(a, typ) and isinstance(b, typ)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 120, in <module>\n",
      "    class Application(SingletonConfigurable):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py\", line 735, in __new__\n",
      "    return super(MetaHasDescriptors, mcls).__new__(mcls, name, bases, classdict)\n",
      "TypeError: super(type, obj): obj must be an instance or subtype of type\n",
      "]\n",
      "[autoreload of IPython.core.ultratb failed: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 368, in superreload\n",
      "    return isinstance(a, typ) and isinstance(b, typ)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 128, in <module>\n",
      "    import IPython.utils.colorable as colorable\n",
      "AttributeError: module 'IPython' has no attribute 'utils'\n",
      "]\n",
      "[autoreload of IPython.utils.PyColorize failed: Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 368, in superreload\n",
      "    return isinstance(a, typ) and isinstance(b, typ)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\utils\\PyColorize.py\", line 176, in <module>\n",
      "    class Parser(Colorable):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py\", line 735, in __new__\n",
      "    return super(MetaHasDescriptors, mcls).__new__(mcls, name, bases, classdict)\n",
      "TypeError: super(type, obj): obj must be an instance or subtype of type\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (TypeError('super(type, obj): obj must be an instance or subtype of type',)).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseAsyncIOLoop._handle_events(536, 1)\n",
      "handle: <Handle BaseAsyncIOLoop._handle_events(536, 1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n",
      "    self.asyncio_loop.add_writer(fd, self._handle_events, fd, IOLoop.WRITE)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n",
      "    gen_log.warning(\"Got events for closed stream %s\", fd)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n",
      "    pass\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    def _run_callback(self, callback, *args, **kwargs):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    self._publish_status(u'idle')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 212, in dispatch_shell\n",
      "    \"\"\"Check whether a shell-channel message should be handled\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 132, in set_parent\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 340, in set_parent\n",
      "    This is now a coroutine\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py\", line 585, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py\", line 559, in set\n",
      "    new_value = self._validate(obj, value)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py\", line 591, in _validate\n",
      "    value = self.validate(obj, value)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\traitlets.py\", line 2529, in validate\n",
      "    value = super(Dict, self).validate(obj, value)\n",
      "TypeError: super(type, obj): obj must be an instance or subtype of type\n"
     ]
    }
   ],
   "source": [
    "X = np.ndarray(shape=(num_data, im_width, im_height, im_channel), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file in enumerate(all_files):\n",
    "    X[idx] = cv2.resize(cv2.imread(file), (im_width, im_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.empty(shape=(4000,im_width, im_height, im_channel), dtype=np.float64)\n",
    "X_val = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\n",
    "X_test = np.empty(shape=(200,im_width, im_height, im_channel), dtype=np.float64)\n",
    "\n",
    "y_train = np.empty(4000)\n",
    "y_val = np.empty(200)\n",
    "y_test = np.empty(200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cls in enumerate(classes): \n",
    "    X_test[50*i:50*(i+1)] = X[np.where(Y == i)[0][:50]]\n",
    "    X_val[50*i:50*(i+1)] = X[np.where(Y == i)[0][50:100]]\n",
    "    X_train[1000*i:1000*(i+1)] = X[np.where(Y == i)[0][100:]]\n",
    "    \n",
    "    y_test[50*i:50*(i+1)] = i\n",
    "    y_val[50*i:50*(i+1)] = i\n",
    "    y_train[1000*i:1000*(i+1)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Y \n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features \n",
    "#Shuffle training index\n",
    "train_idxs = np.random.permutation(X_train.shape[0])\n",
    "y_train  = y_train[train_idxs].astype(int)\n",
    "X_train = X_train[train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1)).astype('float64')\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1)).astype('float64')\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1)).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tiny = X_train[100:110].astype('float64')\n",
    "y_tiny = y_train[100:110].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dev = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = X_train[0:num_dev].astype('float64')\n",
    "y_dev = y_train[0:num_dev].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape\", X_train.shape, \"| y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape, \"| y_test shape:\", y_test.shape)\n",
    "print(\"X_val shape\", X_val.shape, \"| y_val shape:\", y_val.shape)\n",
    "print(\"X_dev shape\", X_dev.shape, \"| y_dev shape:\", y_dev.shape)\n",
    "print(\"X_tiny shape\", X_tiny.shape, \"| y_tiny shape:\", y_tiny.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtract out the mean image \n",
    "#first: compute the mean image\n",
    "# mean_image = np.mean(X_train, axis=0) #axis=0. stack horizontally\n",
    "mean_image = 128\n",
    "#Second subtract the mean image from train and test data \n",
    "X_train -= mean_image\n",
    "X_val -= mean_image \n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "X_tiny -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third append the bias dimension using linear algebra trick\n",
    "#Not for net\n",
    "# X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "# X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "# X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "# X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "# X_tiny = np.hstack([X_tiny, np.ones((X_tiny.shape[0], 1))])\n",
    "\n",
    "print('=====STACK BIAS term=====')\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"X_val shape\", X_val.shape)\n",
    "print(\"X_dev shape\", X_dev.shape)\n",
    "print(\"X_tiny shape\", X_tiny.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some images \n",
    "# Make sure that everything when OK\n",
    "classes = ['broadleaf', 'grass', 'soil', 'soybean']\n",
    "n_class = len(classes)\n",
    "samples_per_class = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y, cls in enumerate(classes):\n",
    "    idxes = np.flatnonzero(y == y_train)\n",
    "    idxes = np.random.choice(idxes, samples_per_class, replace = False)\n",
    "    for i, idx in enumerate(idxes):\n",
    "        plt_idx = i * n_class + y + 1\n",
    "        plt.subplot(samples_per_class,n_class, plt_idx)\n",
    "        plt.imshow(X_train[idx].reshape(im_width, im_height, im_channel).astype('uint8'))\n",
    "        if(i==0): plt.title(cls)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet():\n",
    "    def __init__(self, input_size, hidden_size, output_size, std= 1e-4):\n",
    "        '''\n",
    "        std: weight initialization term\n",
    "        W1: first layer weight, shape(D x H) \n",
    "        W2: second layer weight shape(H x C) \n",
    "        C: num_classes(output_size) , H: hidden_size, D: data_dim(input_size) \n",
    "        '''\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def loss(self, X, y = None, reg=0.0):\n",
    "        '''\n",
    "        reg: regularization strength\n",
    "        X: ndarray shape(N x C). N: num of data \n",
    "        y: vector of training label\n",
    "        '''\n",
    "        #DEfine relu activation function \n",
    "        relu = lambda x:np.maximum(0,x)\n",
    "\n",
    "        #unpack\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        #Forward prop\n",
    "        layer1 = relu(X.dot(W1) + b1)  #(N,D) x (D,H) = (N,H)\n",
    "        scores = layer1.dot(W2) + b2\n",
    "\n",
    "        #if target is not given then jump out \n",
    "        if(y is None): \n",
    "            return scores\n",
    "\n",
    "        #compute the loss \n",
    "        ##Normalization trick to prevent overflow when compute exp \n",
    "        scores -= scores.max()#stack vertically\n",
    "\n",
    "        scores = np.exp(scores)\n",
    "        scores_sumexp = np.sum(scores, axis=1)#stack vertically\n",
    "\n",
    "        ##Nomalize all score \n",
    "        softmax = scores / scores_sumexp.reshape(N,1)  #Shape: (N, C)\n",
    "        #total loss of all training. -log of all correct score\n",
    "        loss =  (-1.0) * np.sum(np.log(softmax[range(N),y]))\n",
    "\n",
    "        ##Normalize the loss and add regularization strength \n",
    "        loss /= N \n",
    "        loss += reg * np.sum(W1 * W1) \n",
    "        loss += reg * np.sum(W2 * W2) \n",
    "\n",
    "        #Backward pass on the net \n",
    "        grads = {}\n",
    "\n",
    "        correct_class_scores = scores[range(N), y]\n",
    "        softmax[range(N), y] = (-1.0) * (scores_sumexp - correct_class_scores)/scores_sumexp\n",
    "        softmax /= N\n",
    "\n",
    "\n",
    "        #Want to find dW2(dL/dW2)\n",
    "        # Derivation: dL/dW2 = dL/dscore * dscore/dW2(chain rule)\n",
    "        #dL/dscore = softmax since L(score) = softmax(variable)\n",
    "        #dscore/dW2 = relu_(hidden layer output)\n",
    "        grads['W2'] = layer1.T.dot(softmax)\n",
    "        grads['b2'] = np.sum(softmax, axis=0)#stack horizontally\n",
    "        grads['W2'] += reg * 2 * W2\n",
    "\n",
    "        #dL/dW1 = dL/dscore * dscore/drelu(layler1) * drelu(layer1)/dW1 \n",
    "        #dL/dW1 = dW1 = softmax * W2 * X \n",
    "        hidden = softmax.dot(W2.T)\n",
    "\n",
    "        #derivative of a max gate\n",
    "        #Intuition: in forward pass if neuron didn't fire that mean. the derivative of that neuron \n",
    "        # is 0. This might be bad since this will kill gradient. \n",
    "        hidden[layer1 == 0] = 0 \n",
    "\n",
    "        grads['W1'] = X.T.dot(hidden) \n",
    "        grads['b1'] = np.sum(hidden, axis=0) #stack horizontally \n",
    "        grads['W1'] += reg * 2 * W1\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, \n",
    "              learning_rate =1e-3, learning_rate_decay=0.95, \n",
    "              reg=5e-6, num_iters=100, \n",
    "              batch_size=200, it_verbose = 1, verbose=False):\n",
    "        '''\n",
    "        Train using SGD \n",
    "        Input: \n",
    "            X: nd array shape(N x D) \n",
    "            y: vector of train label \n",
    "            X_val: nd array shape( n_VAL , D) Use as validation set after each epoch \n",
    "            y_val: vector of validation label \n",
    "        '''\n",
    "        N, D = X.shape\n",
    "        N_val = X_val.shape[0]\n",
    "        iteration_per_epoch = max(N/batch_size, 1)\n",
    "        \n",
    "        loss_hist = []\n",
    "        train_acc_hist = []\n",
    "        val_acc_hist = []\n",
    "        \n",
    "        for it in range(num_iters):\n",
    "            sampling = np.random.choice(np.arange(N), batch_size, replace=False) \n",
    "            X_batch = X[sampling]\n",
    "            y_batch = y[sampling]\n",
    "            \n",
    "            #compute loss and gradients\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_hist.append(loss) \n",
    "            \n",
    "            #Update rule \n",
    "            self.params['W1'] += (-1.0) * learning_rate * grads['W1']\n",
    "            self.params['b1'] += (-1.0) * learning_rate * grads['b1']\n",
    "            self.params['W2'] += (-1.0) * learning_rate * grads['W2']\n",
    "            self.params['b2'] += (-1.0) * learning_rate * grads['b2']\n",
    "            \n",
    "            if(verbose and it%it_verbose==0):\n",
    "                print('iteration: %d / %d | Loss: %f' % (it, num_iters, loss)) \n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if (it % iteration_per_epoch == 0):\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_hist.append(train_acc)\n",
    "                val_acc_hist.append(val_acc)\n",
    "\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "        return {\n",
    "            'loss_hist':loss_hist,\n",
    "            'train_acc_hist':train_acc_hist,\n",
    "            'val_acc_hist':val_acc_hist\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict labels for\n",
    "        data points. For each data point we predict scores for each of the C\n",
    "        classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "        classify.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "        the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "        to have class c, where 0 <= c < C.\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        relu = lambda x:np.maximum(0,x)\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "\n",
    "        #Forward propagation though the network \n",
    "        layer1 = relu(X.dot(W1) + b1)\n",
    "        scores = layer1.dot(W2) + b2 #shape: (N x C)\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = im_width * im_height * im_channel\n",
    "hidden_size = 200\n",
    "output_size = n_class \n",
    "std = 1e-3 # size initialization parameter\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size,output_size,std )\n",
    "stats = net.train(X_dev, y_dev, X_val, y_val, \n",
    "              learning_rate =1e-5, learning_rate_decay=0.95, \n",
    "              reg=0.0, num_iters=70, \n",
    "              batch_size=100, it_verbose = 10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss history and train/ validation accuracies history\n",
    "plt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\n",
    "plt.subplot(2,1,1) \n",
    "plt.plot(stats['loss_hist'])\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(stats['train_acc_hist'], label='train')\n",
    "plt.plot(stats['val_acc_hist'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classfication Accuracies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((net.predict(X_test) == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "best_val= -1 #highest validation accuracy\n",
    "\n",
    "\n",
    "hidden_unit = [200]\n",
    "learn_rates = [7.6e-5]\n",
    "regularizations = [0.0]\n",
    "iterations = [600]\n",
    "\n",
    "result = {}\n",
    "best_stats = None\n",
    "\n",
    "input_size = im_height * im_width * im_channel # \n",
    "output_size = 4 #4 class\n",
    "for hidden in hidden_unit: \n",
    "    for learn in learn_rates:\n",
    "        for r in regularizations:\n",
    "            for iter in iterations:\n",
    "                tune_net = TwoLayerNet(input_size,\n",
    "                                       hidden_size=hidden,\n",
    "                                       output_size=output_size,std=1e-3)\n",
    "                stats = tune_net.train(X_train, y_train, X_val, y_val, \n",
    "                              num_iters=iter, batch_size=200, \n",
    "                              learning_rate=learn,learning_rate_decay=0.94, \n",
    "                              reg=r,  it_verbose = 100,verbose=True)\n",
    "                train_acc = stats['train_acc_hist'][-1]#get last value \n",
    "                val_acc = stats['val_acc_hist'][-1]\n",
    "                result[(hidden, learn)] = (train_acc, val_acc)\n",
    "                #print log\n",
    "                print('hs:',hidden,'learn:',learn,'reg',r,'iter',iter,'train-acc:',train_acc,'val_acc',val_acc)\n",
    "                if(val_acc > best_val):\n",
    "                    best_val = val_acc\n",
    "                    #create best net\n",
    "                    best_stats = stats\n",
    "                    best_net = tune_net\n",
    "                del tune_net\n",
    "                del stats\n",
    "\n",
    "print(\"Accuracy on Test set\", (best_net.predict(X_test) == y_test).mean())         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss history and train/ validation accuracies history\n",
    "plt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\n",
    "plt.subplot(2,1,1) \n",
    "plt.plot(best_stats['loss_hist'])\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(best_stats['train_acc_hist'], label='train')\n",
    "plt.plot(best_stats['val_acc_hist'], label='val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classfication Accuracies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check prediction statistic of the model\n",
    "\n",
    "    - In this case, the model did learn each class uniformly. Predictions are uniformly distributed to all classes.\n",
    "    - One conclude that the model learn all classes evenly. :D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(best_net.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
